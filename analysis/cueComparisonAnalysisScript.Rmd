---
title: "Exploring Social Metacognition - Advisor/Cue comparison"
author: Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)
output: 
  html_document:
    toc: false
    toc_depth: 3
    includes:
      after_body: src/toc_menu.html
editor_options: 
  chunk_output_type: console
---
December 2018  
[Script run `r Sys.time()`]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
startTime <- Sys.time()
source('src/ESM_core.R')
```

# Descriptives 

## Introduction 

Participants performed a perceptual discrimination task in a judge-advisor system. Participants were randomly assigned to receive advice either from 'Advisors' or 'Cues'. The two advisor classes are tested for differences, particularly in pick rates and influence within pairs of advisors which differ in the frequency with which their advice agrees with the participant's initial decision.

<div style="display: flex; justify-content: space-between">
  <div style="text-align: center">![Advisor advice image](img/advisor_advice.png)<br/>
      Advisor advice
  </div>
  <div style="text-align: center">![Cue advice image](img/cue_advice.png)<br/>
      Cue advice
  </div>
</div>

## Load data  

```{r}
# folderName <- "G:\\Documents\\University\\Google Drive\\Temp\\data\\processed"

# results <- loadFilesFromFolder(folderName)
# results <- removeParticipantIds(results)

folderName <- 'ESM_sim.R'
source('src/ESM_sim.R')
results <- simulateAdvisorChoice(16, aPairs = list('agr' = c(7,8)), tTypes = c(trialTypes$dual, trialTypes$change))

# unpack results
for(i in 1:length(results))
  assign(names(results)[i], results[i][[1]])

cat(paste('Loaded data from', folderName))
```

```{r}
trials <- cbind(trials, trialUtilityVariables(results))
all.trials <- trials
trials <- trials[trials$practice == F, ]
cat('Generated utility variables')
```

## Describe data 

### Metadata

Responses (`r paste('*N* =', length(unique(participants$pid)))`) were collected between `r as.POSIXct(min(unlist(participants$timeStart))/1000, tz = '', origin = '1970-01-01')` and `r as.POSIXct(max(unlist(participants$timeEnd))/1000, tz = '', origin = '1970-01-01')`.

Demographic data are not collected and therefore not analysed. Participants must be over 18 years old to use the Prolific recruitment platform.

### Task performance

#### Type 1 performance

Participants initial performance was held at 71% by design. Participants' mean percentage correct on initial decisions was `r md.mean(aggregate(initialCorrect ~ pid, trials, mean)$initialCorrect, isProportion = T)`. We would expect final decisions to be more accurate due to the presence of advice: 

```{r results = 'asis'} 
tmp <- aggregate(cbind(initialCorrect, finalCorrect) ~ pid, trials, mean) 
cat(md.ttest(tmp$initialCorrect, tmp$finalCorrect, 
             labels = c('*M*|initial', '*M*|final'), 
             isProportion = T, paired = T))
```

Type 1 performance should not differ in initial decisions as a function of advisor class (Advisor vs Cue). The final decisions might differ if advice taking differs between the advisor classes.

Initial decisions: 

```{r results = 'asis'}
m <- tmp$pid %in% participants$pid[participants$advisorClass == "Advisor"]
tmp <- aggregate(cbind(initialCorrect, finalCorrect) ~ pid, trials, mean) 

cat(md.ttest(tmp$initialCorrect[m], tmp$initialCorrect[!m], 
             labels = c('*M*|Advisor', '*M*|Cue'), 
             isProportion = T))
```

Final decisions:

```{r results = 'asis'}
cat(md.ttest(tmp$finalCorrect[m], tmp$finalCorrect[!m], 
             labels = c('*M*|Advisor', '*M*|Cue'), 
             isProportion = T))
```

#### Type 2 performance

Type 2 (metacognitive) performance is characterised using Type 2 ROC. The mean ROC for all participants is tabulated below:

```{r results = 'asis'}
df.type2 <- NULL
for(p in unique(trials$pid)) {
  for(d in c('initial', 'final')) {
    tmp <- trials[trials$pid == p, c(paste0(d, 'Correct'), paste0(d, 'Confidence'))]
    # remove NA values which appear in final judgements which are never made
    tmp <- tmp[!is.na(tmp[ ,1]), ]
    roc <- type2ROC(tmp[ ,1], tmp[ ,2], bins = 7)
    df.type2 <- rbind(df.type2, data.frame(pid = factor(p), decision = d, conf = roc$x, pCorrect = roc$y))
  }
}
tmp <- seq(0, 1, length.out = length(unique(df.type2$conf)))
tmp <- sapply(1:(length(tmp)-1), function(i) mean(c(tmp[i], tmp[i+1])))
df.type2$confProp <- sapply(df.type2$conf, function(x) tmp[which(levels(df.type2$conf) == x)])

tmp <- aggregate(pCorrect ~ conf + decision, df.type2, mean)
# print neatly with rounding
tmp[ ,3] <- prop2str(tmp[ ,3])
kable(prop2str(tmp))
```

Participants' ROC curves:

```{r}
ggplot(df.type2, aes(x = confProp, y = pCorrect, colour = pid)) +
  geom_abline(slope = 1, intercept = c(0,0), linetype = 'dashed', colour = 'black') +
  geom_point() +
  geom_line(alpha = .5, aes(group = pid)) +
  facet_wrap(~decision, labeller = label_both) +
  scale_x_continuous(limits = c(0,1)) +
  coord_fixed() +
  style.long +
  theme(panel.spacing.x = unit(1, 'lines'))
```

### Exclusions

```{r}
participants$excluded <- sapply(participants$pid, function(pid){
  ts <- all.trials[all.trials$pid == pid,]
  # overall accuracy of initial decisions
  m <- mean(ts$initialCorrect, na.rm = T)
  if(m < .6 | m > .85) return('Accuracy')
  # varied use of confidence scale
  cCs <- aggregate(pid ~ confidenceCategory, data = ts, FUN = length)
  # All confidence categories must be used
  if(nrow(cCs) < 3) return ('Confident')
  # All confidence categories must have at least 5% of the number of trials
  if(any(cCs$pid < length(ts)*.05)) return('<5%')
  return(F)
  })
# exclude on the basis of collecting too much data
if(sum(participants$excluded == F) > 50) {
  tmp <- participants[participants$excluded == F, c('id', 'timeStart')]
  tmp <- tmp$id[order(tmp$timeStart)]
  tmp <- tmp[1:50]
  participants$excluded[!(participants$id %in% tmp)] <- 'Excess'
}
```

We exclude participants for:

* Proportion of correct initial judgements must be (.60 < cor1/n < .90) (*N* = `r sum(participants$excluded == 'Accuracy')`)

* Having fewer than 3 confidence categories (*N* = `r sum(participants$excluded == 'Confidence')`)

* Having fewer than 5% of trials in each confidence category (*N* = `r sum(participants$excluded == '<5%')`)

* There being more data collected than specified in pre-registration (*N* = `r sum(participants$excluded == 'Excess')`)

*NB: **practice trials are included** in this since they are used in part for determining confidence calibration*

The number of participants analysed after exclusions (total *N* = `r sum(participants$excluded != F)`) have taken place is `r sum(participants$excluded == F)`.

```{r}
# Perform exclusions
participants <- participants[participants$excluded==F, ]
# Remove excluded participants' data from other data frames
all.trials <- all.trials[all.trials$pid %in% participants$pid, ]
trials <- trials[trials$pid %in% participants$pid, ]
advisors <- advisors[advisors$pid %in% participants$pid, ]
questionnaires <- questionnaires[questionnaires$pid %in% participants$pid, ]
genTrustQ <- genTrustQ[genTrustQ$pid %in% participants$pid, ]
```

### Advisor performance

#### Manipulation checks

The advisors need to differ appropriately. How they differ depends on the advisors being compared, but in each case we need to check the participants' experience of the advisors matched the specifications for the advisors.

```{r results = 'asis'}
tmp <- advisorManipulationData(trials)
l.advisorDiff <- tmp
tmp <- prop2str(l.advisorDiff$summary, 3)
rownames(tmp) <- rownames(l.advisorDiff$summary)
kable(tmp)

# significane tests
for(i in 1:ncol(l.advisorDiff$data[ ,-1])) {
  if(i %% 2 == 1) {
    cat('\n\n')
    next()
  }
  cat(md.ttest(l.advisorDiff$data[ ,i], l.advisorDiff$data[ ,i+1], 
                 labels = paste0('*M*|', names(l.advisorDiff$data[i:(i+1)])), 
                 isProportion = T, paired = T)) 
}
```

This experience should not differ by advisor class for either advice type:

```{r results = 'asis'}
m <- l.advisorDiff$data$pid %in% participants$pid[participants$advisorClass == "Advisor"]
for(i in 2:ncol(l.advisorDiff$data)) {
  cat(md.ttestBF(l.advisorDiff$data[m,i], l.advisorDiff$data[!m,i], 
                 labels = paste0('*M*|', names(l.advisorDiff$data[i]), c('|Advisor', '|Cue')), 
                 isProportion = T)) 
  cat('\n\n')
}
```

#### Initial decision comparability

The participants' initial decisions should be equivalent (BF < .333) between advisors. Comparing initial decision by advice profile we can see whether this was the case:

```{r results = 'asis'} 
tmp <- aggregate(initialCorrect ~ pid + adviceType, trials, mean) 
# sort through using reference advisors
for(a in adviceTypes[unlist(adviceTypes) %% 2 == 1 & unlist(adviceTypes) %in% trials$adviceType]) {
  cat(md.ttestBF(tmp$initialCorrect[tmp$adviceType == a], 
                 tmp$initialCorrect[tmp$adviceType == a+1], 
                 labels = paste0('*M*|', getAdviceTypeName(c(a, a+1))),
                 isProportion = T, paired = T))
  cat('\n\n')
}
```

# Pick rate differences

The primary outcome is the relative frequencies of advisor picking on trials where there is a choice of advisor. Choices occur on *change* trials which provide a default advisor and allow the participant to hear advice from the other advisor instead by pressing a key within a selection window (`r unique(participants$changeDuration)`ms). Critically, the proportion of trials on which each advisor is the default is balanced.

## Change trials

Change trials are analysed in two ways. First, by inspecting whether the proportion of trials on which advice was received from the reference advisor differs by advisor class:

```{r results = 'asis'}
df.change <- NULL
pairs <- getAdviceTypePairs(c(trials$advisor0id, trials$advisor1id))
if(length(pairs) < 1) {
  cat('*No change trials in the current dataset.*')
} else {
  for(pair in pairs) {
    tmp <- trials[trials$type == trialTypes$change, ]
    refAdvisor <- pair[1]
    tmp <- aggregate(advisorId ~ pid, data = tmp, function (x) mean(x == refAdvisor))
    m <- tmp$pid %in% participants$pid[participants$advisorClass %in% "Advisor"]
    cat(md.ttest(tmp$advisorId[m], tmp$advisorId[!m], 
                 labels = paste0('*P*(', getAdviceTypeName(refAdvisor), c('|Advisor)', '|Cue)')), 
                 isProportion = T))
    cat('\n\n')
    if(is.null(df.change))
      df.change <- tmp
    else
      df.change <- cbind(df.change, tmp[ ,-1])
    colnames(df.change)[ncol(df.change)] <- getAdviceTypeName(refAdvisor)
  }
}
```

Second, by comparing the proportion of changes which favoured the reference advisor by advisor class:

```{r results = 'asis'}
df.changeOnly <- NULL
if(length(pairs) < 1) {
  cat('*No change trials in the current dataset.*')
} else {
  for(pair in pairs) {
    tmp <- trials[trials$type == trialTypes$change & trials$advisorId != trials$defaultAdvisor, ]
    refAdvisor <- pair[1]
    tmp <- aggregate(advisorId ~ pid, data = tmp, function (x) mean(x == refAdvisor))
    m <- tmp$pid %in% participants$pid[participants$advisorClass %in% "Advisor"]
    cat(md.ttest(tmp$advisorId[m], tmp$advisorId[!m], 
                 labels = paste0('*P*(', getAdviceTypeName(refAdvisor), c('|Advisor)', '|Cue)')), 
                 isProportion = T))
    cat('\n\n')
    if(is.null(df.changeOnly))
      df.changeOnly <- tmp
    else
      df.changeOnly <- cbind(df.changeOnly, tmp[ ,-1])
    colnames(df.changeOnly)[ncol(df.changeOnly)] <- getAdviceTypeName(refAdvisor)
  }
}
```

<div style="display: flex; justify-content: space-between; flex-wrap: wrap;">
  <div>
Graph for advice received:

```{r}
if(!is.null(df.change)) {
  df.change$advisorClass <- participants$advisorClass[participants$pid == df.change$pid]
  df.change$nChanges <- sapply(df.change$pid, function(x) nrow(trials[trials$pid == x 
                                                                      & trials$type == trialTypes$change 
                                                                      & trials$defaultAdvisor != trials$advisorId,]))
  tmp <- gather(df.change, key = "refAdvisor", value="pickRate", -(c(1, ncol(df.change)-1, ncol(df.change))))
  
  ggplot(tmp, aes(x = refAdvisor, y = pickRate, fill = advisorClass)) +
    geom_violin(colour = NA, alpha = .5) +
    geom_point(position = position_jitterdodge(jitter.width = .1, dodge.width = .9), alpha = 1,
               aes(colour = factor(pid), size = nChanges)) +
    stat_summary(geom = 'point', aes(group = advisorClass), fun.y = mean, shape = 18, size = 5,
                 position = position_dodge(.9)) +
    stat_summary(geom = 'errorbar', aes(group = advisorClass), fun.data = mean_cl_normal, width = .25,
                 position = position_dodge(.9)) +
    scale_y_continuous(limits = c(0,1)) +
    labs(y = 'P(Gave advice)', x = "advisor", 
         subtitle = "Violins show distributions for Advisors (pink) and Cues (blue).\nDots show means for individual participants,\nwith the size inidicating the number of changes made.") + 
    style.long 
}
```
  </div>
  <div>
Graph for the changes only:

```{r}
if(!is.null(df.changeOnly)) {
  df.changeOnly$advisorClass <- participants$advisorClass[participants$pid == df.changeOnly$pid]
  df.changeOnly$nChanges <- sapply(df.changeOnly$pid, function(x) df.change$nChanges[df.change$pid == x])
  tmp2 <- gather(df.changeOnly, key = "refAdvisor", value="pickRate", -(c(1, ncol(df.change)-1, ncol(df.change))))
  
  ggplot(tmp2, aes(x = refAdvisor, y = pickRate, fill = advisorClass)) +
    geom_violin(colour = NA, alpha = .5) +
    geom_point(position = position_jitterdodge(jitter.width = .1, dodge.width = .9), alpha = 1,
               aes(colour = factor(pid), size = nChanges)) +
    stat_summary(geom = 'point', aes(group = advisorClass), fun.y = mean, shape = 18, size = 5,
                 position = position_dodge(.9)) +
    stat_summary(geom = 'errorbar', aes(group = advisorClass), fun.data = mean_cl_normal, width = .25,
                 position = position_dodge(.9)) +
    scale_y_continuous(limits = c(0,1)) +
    labs(y = 'P(Changed to)', x = "advisor",
         subtitle = "Violins show distributions for Advisors (pink) and Cues (blue).\nDots show means for individual participants,\nwith the size inidicating the number of changes made.") +
    style.long
}
```
  </div>
</div>

For completeness we also check the proportion of trials on which the reference advisor is the default advisor is equivalent to .5.

```{r results = 'asis'}
if(length(pairs) < 1) {
  cat('*No change trials in the current dataset.*')
} else {
  for(pair in pairs) {
    refAdvisor <- pair[1]
   # Calculate default advisor = reference advisor proportions for each participant
    defaults <- sapply(unique(trials$pid), function(pid) {
      tmp <- trials[trials$pid == pid & trials$type == trialTypes$change, ]
      tmp <- tmp$defaultAdvisor[tmp$defaultAdvisor %in% pair]
      return(mean(tmp == refAdvisor, na.rm = T))
    })
    
    cat(md.ttestBF(defaults, mu = .5, labels = getAdviceTypeName(refAdvisor), isProportion = T))
    cat('\n\n')
  }
}
```

# Influence differences  

Influence, defined as the extent to which a participant adjusts their answer in the direction of an advisor's advice, can be compared between advisors. Note that the experimental design is not optimised for measuring this outcome because influence may vary systematically with choices, and the trials without choices constitute a learning phase wherein preferences cannot be expected to have crystalised. 

For dual trials the confidence shift is modelled using a linear model in which the initial confidence and the agreement of each advisor (and their interaction) are used as predictors:

```{r}
tmp <- trials[trials$type == trialTypes$dual, ]

for(pair in getAdviceTypePairs(c(tmp$advisor0type, tmp$advisor1type))) {
  aNames <- paste0(getAdviceTypeName(pair), '_agrees')
  # rearrange data so advisors are separated by type rather than order
  df <- tmp[tmp$advisor0type %in% pair & tmp$advisor1type %in% pair, ]
  df[ , aNames[1]] <- ifelse(df$advisor0type == pair[1], df$advisor0agrees, df$advisor1agrees)
  df[ , aNames[2]] <- ifelse(df$advisor0type == pair[2], df$advisor0agrees, df$advisor1agrees)
  
  df$advisorClass <- sapply(df$pid, function(x) participants$advisorClass[participants$pid == x])
  
  if(nrow(df) == 0)
    next()
  
  print(paste('LM results for', getAdviceTypeName(pair[1], long = T),
              'vs', getAdviceTypeName(pair[2], long = T)))
  
  f <- as.formula(paste0("confidenceShift ~ pid + initialConfidence + (", aNames[1],
                        ' * ', aNames[2], ' * advisorClass)'))
  model <- lm(f, data = df)
  print(summary(model))
}
```


# Exploration 

# Credits 

## Acknowledgements

Thanks as always to Nick Yeung and the other folks at the [ACC Lab](https://www.psy.ox.ac.uk/research/attention-cognitive-control-lab).

## R Packages

```{r results = 'asis'}
# list packages
packageNames <- (.packages())
# don't include very core package
packageNames <- packageNames[!(packageNames %in% rownames(installed.packages(priority="base")))]
# but do include the base package
packageNames <- c("base", packageNames)
out <- NULL
for(p in packageNames) {
  out <- rbind(out, data.frame('Package' = p, 
                               'Citations' = paste(format(citation(p), style = 'textVersion'), 
                                                   collapse = '<br/><br/>')))
}

kable(out)
```

## Funding

Matt Jaquiery is funded by a studentship from the [Medical Research Council](https://mrc.ukri.org/) (reference 1943590) and the University of Oxford [Department of Experimental Psychology](https://www.psy.ox.ac.uk/) (reference 17/18_MSD_661552).

## Technical details  

```{r results = 'hold'}
cat(paste('Time stamp:', Sys.time(), '\n\n'))
cat('Runtime \n')
proc.time()
cat('\n')
sessionInfo()
```